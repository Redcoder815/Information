round((df.isnull().sum() / df.shape[0]) * 100, 2)
df.shape[0]
df.shape[0] is the number of rows in the DataFrame.
Dividing the missing counts by the number of rows gives the fraction of missing values per column
------------------------------
plt.boxplot(df1['Age'], vert=False)
vert false means box plot will not be vertical.it will be horizontal
-------------------------
min max scaling

https://medium.com/@iamkamleshrangi/how-min-max-scaler-works-9fbebb9347da
----------------------------------------

fig, axes = plt.subplots(len(df.columns), 1, figsize=(7, 14), dpi=95)
len(df.columns) is number of row of axes
dpi=95
Dots Per Inch — controls the resolution of the figure.
Higher DPI → sharper image (more pixels per inch).
95 is a moderate resolution, good for on-screen display.
-----------------------------------

np.percentile(df['Insulin'], [25, 75])

Instead of a single number like 25, you give a list [25, 75].

That tells NumPy: “Give me both the 25th and the 75th percentile at once.”
----------------------------------

sns.heatmap(corr, annot=True, fmt='.2f', cmap='coolwarm')
The parameter annot=True means:

Display the numerical values inside each cell of the heatmap.
-----------------------------------
print(corr['Outcome'].sort_values(ascending=False))

3. .sort_values(ascending=False)
This sorts the Series by its values.
ascending=False means highest correlation first.
Sorting helps you quickly see which variables are most positively correlated with "Outcome".

Correlation coefficients range from -1.0 to 1.0:
1.0 → perfect positive correlation
0.0 → no correlation
-1.0 → perfect negative correlation
If you sort descending (ascending=False), you get:
Highest positive correlations first (close to +1)
Then smaller positive correlations
Then negative correlations (close to -1) at the bottom

---------------------------------------

df = df.select_dtypes(include=np.number)
include = np.number for keeping numeric column only

import pandas as pd
import numpy as np

# Sample DataFrame
df = pd.DataFrame({
    'A': [1, 2, 3],          # int
    'B': [4.5, 5.5, 6.5],    # float
    'C': ['x', 'y', 'z']     # string
})

# Keep only numeric columns
df = df.select_dtypes(include=np.number)

print(df)
-------------------------------
max_abs = np.max(np.abs(df), axis=0)

axis=0 → collapse rows → result is per column.
axis=1 → collapse columns → result is per row.
---------------------------------------
scaled_df = pd.DataFrame(scaled_data, columns=df.columns)
the columns=df.columns part is telling Pandas to give the new DataFrame the same column names as the original df.
------------------------------------------------------------
df_encoded = pd.get_dummies(df, columns=['Color'], prefix='Color')
When you set prefix='Color', Pandas will prepend "Color_" to each new column name.
--------------------------------------------

In pandas.cut(), the parameter right controls whether the bins include the right edge of the interval.

Meaning of right=False
right=True (default) → bins are right-closed: [ )
Example: (10, 20] means 10 < x ≤ 20
right=False → bins are left-closed: [ )
Example: [10, 20) means 10 ≤ x < 20
----------------------------------------

import nltk
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from sklearn.feature_extraction.text import CountVectorizer

texts = ["This is a sample sentence.", "Text data preprocessing is important."]

stop_words = set(stopwords.words('english'))
stemmer = PorterStemmer()
vectorizer = CountVectorizer()


def preprocess_text(text):
    words = text.split()
    words = [stemmer.stem(word)
             for word in words if word.lower() not in stop_words]
    return " ".join(words)


cleaned_texts = [preprocess_text(text) for text in texts]

X = vectorizer.fit_transform(cleaned_texts)

print("Cleaned Texts:", cleaned_texts)
print("Vectorized Text:", X.toarray())
----------------------------
enumerate(iterable, start=0)
iterable — the sequence you want to loop over.
start — the index value to begin counting from (default is 0).
So in your code:

Python

Copy code
for idx, feature in enumerate(numerical_columns, 1):
the 1 means:

The counter idx will start at 1 instead of 0
--------------------------------
plt.subplot(nrows, ncols, index)
nrows – number of rows in the subplot grid
ncols – number of columns in the subplot grid
index – position of the subplot you want to create (starting at 1, not 0)
When idx = 1 → first subplot (row 1, col 1)
When idx = 2 → second subplot (row 1, col 2)
When idx = 3 → third subplot (row 2, col 1)
--------------------------------------------
5. .skew()
This is a Pandas Series method that calculates the skewness of the data in that column.
Skewness measures the asymmetry of the data distribution:
0 → perfectly symmetrical
Positive → tail on the right
Negative → tail on the left
Symmetrical → Skewness close to 0
Positive Skew → Skewness > 0
Negative Skew → Skewness < 0
--------------------------------
sns.heatmap(df.corr(), annot=True, fmt='.2f', cmap='Pastel2', linewidths=2)
means each cell in the heatmap will be separated by a 2-point-wide line.
-----------------------------

df = pd.read_csv("/content/stock_data.csv", parse_dates=True, index_col="Date")
the argument parse_dates=True tells pandas to try to automatically detect and convert any columns that look like dates into datetime64[ns] objects instead of leaving them as plain strings.

------------------------------------------------------
plot_acf(df['High'], lags=40)
the lags parameter tells statsmodels.graphics.tsaplots.plot_acf how many time lags to include in the autocorrelation plot.

Meaning of lags
Lag = how far back in time you compare the series with itself.
Lag 1: correlation between today’s value and yesterday’s value.
Lag 2: correlation between today’s value and the value 2 time steps ago.
lags=40 means:
Compute and plot autocorrelations for lags 1 through 40 (plus lag 0, which is always correlation = 1).
Example
If your data is daily stock prices in df['High']:

Lag 1: correlation between today’s high and yesterday’s high.
Lag 40: correlation between today’s high and the high 40 days ago.
Notes
You can pass:
An integer → number of lags to plot.
A list/array → specific lags to plot (e.g., lags=[1, 5, 10, 20]).
Choosing lags depends on your data frequency and the patterns you want to detect (seasonality, persistence, etc.).
✅ In short: lags=40 means “show autocorrelation values for up to 40 time steps back.”
-------------------------------------

The Augmented Dickey-Fuller (ADF) test is a popular statistical test used to determine if a time series is stationary. In Python, the adfuller() function from the Statsmodels library makes it easy to perform this test.

If p-value < 0.05 → reject 
H
0
H 
0
​
  → series is likely stationary.
If p-value >= 0.05 → fail to reject 
H
0
H 
0
​
  → series is likely non-stationary.

--------------------------------------
window_size = 120
df['high_smoothed'] = df['High'].rolling(window=window_size).mean()

1. window_size = 120
This sets a variable window_size to the integer 120.
In the context of rolling calculations, this means we will look at 120 consecutive rows at a time.
If your data is time-series (e.g., daily prices), 120 could mean 120 days.
window=120 means:
For each row, Pandas will look backwards at the current row and the previous 119 rows (total 120 rows).
At the start of the dataset, where fewer than 120 rows exist, the result will be NaN unless you specify min_periods.
This does not calculate anything yet — it just prepares the rolling object

---------------------------------------------------

from sklearn.datasets import make_regression
import numpy as np

# Low noise
X1, y1 = make_regression(n_samples=5, n_features=1, noise=0.1, random_state=42)
# High noise
X2, y2 = make_regression(n_samples=5, n_features=1, noise=50, random_state=42)

print("Low noise y:", np.round(y1, 2))
print("High noise y:", np.round(y2, 2))
Low noise → model fits data more accurately, coefficients are closer to the true values.
High noise → model may shrink more coefficients to zero or misestimate them, because the signal is harder to detect.
-----------------------------------------------
How alpha affects the model
Think of alpha as a knob that controls how much you punish large coefficients:

alpha value	Effect on model	Coefficients	Bias / Variance
0	No penalty → behaves like normal linear regression	All kept	Low bias, high variance
Small (e.g., 0.01)	Light penalty	Slightly shrunk	Slight bias, reduced variance
Medium (e.g., 0.1)	Moderate penalty	Some coefficients become exactly zero	More bias, less variance
Large (e.g., 10)	Strong penalty	Many coefficients zero → simpler model	High bias, very low variance
Intuition
Small alpha → Model fits data closely, but may overfit.
Large alpha → Model is simpler, may underfit, but generalizes better.
Right alpha → Balances bias and variance for best predictive performance.
--------------------------------------

X, y = make_regression(n_samples=100, n_features=5, noise=0.1, random_state=42)
n_features 5 means 5 columns
------------------------------------

In Elastic Net regression, the parameter l1_ratio controls the balance between L1 (Lasso) and L2 (Ridge) regularization.

Meaning of l1_ratio
l1_ratio = 1.0 → Pure Lasso (only L1 penalty: encourages sparsity, can set coefficients exactly to zero).
l1_ratio = 0.0 → Pure Ridge (only L2 penalty: shrinks coefficients but keeps all features).
0 < l1_ratio < 1 → Mix of L1 and L2 penalties.
--------------------------------------

In Seaborn's heatmap function, the fmt parameter controls how the annotation text is formatted when annot=True.

Common fmt options in Seaborn (from Matplotlib’s format strings):
'g' → General format (default, minimal digits, no fixed decimals unless needed)
'.2f' → Fixed-point with 2 decimal places (e.g., 3.14)
'.0f' → Fixed-point with no decimals (rounded integer)
'.2%' → Percentage with 2 decimal places (e.g., 0.45 → 45.00%)
'd' → Integer format (for integer data only)
'.3e' → Scientific notation with 3 decimals
----------------------------------------
pad=20 → adds 20 points of vertical space between the title and the top of the plot area.
plt.title('Confusion Matrix', fontsize=17, pad=20)
----------------------------------------------

plt.gca().xaxis.set_label_position('top') 
plt.gca() → Gets the current Axes object from Matplotlib.
.xaxis → Accesses the X-axis object of that Axes.
.set_label_position('top') → Moves the X-axis label (not the ticks) to the top of the plot.
------------------------------------------

It moves the x-axis ticks and labels to the top of the plot instead of the default bottom.
plt.gca().xaxis.tick_top()
----------------------------------------
bottom=0.2 means the bottom edge of the plot area starts 20% of the figure height from the bottom of the figure.
plt.gca().figure.subplots_adjust(bottom=0.2)
-------------------------------------

import matplotlib.pyplot as plt

fig, ax = plt.subplots()

# Add three texts with different horizontal alignments
fig.text(0.5, 0.8, 'Left aligned', ha='left', fontsize=12, color='red')
fig.text(0.5, 0.6, 'Center aligned', ha='center', fontsize=12, color='green')
fig.text(0.5, 0.4, 'Right aligned', ha='right', fontsize=12, color='blue')

plt.show()
the parameter ha stands for horizontal alignment of the text.

Possible values for ha in Matplotlib:
"left" – Aligns the text so that its left edge is at the given (x, y) position.
"center" – Aligns the text so that its center is at (x, y).
"right" – Aligns the text so that its right edge is at (x, y).
----------------------------------
plt.cm.Blues is a sequential colormap that goes from light to dark blue.
disp.plot(cmap=plt.cm.Blues)
it give color to each cell of matrix.
---------------------------------------

“Give me the F1‑score for each class separately, not a single combined score.”
So instead of one number, you get an array:
[F1_for_class_0, F1_for_class_1, F1_for_class_2, ...]
from sklearn.metrics import f1_score

y_true = [0, 1, 2, 2, 1, 0]
y_pred = [0, 2, 2, 2, 1, 1]
f1_per_class = f1_score(y_true, y_pred, average=None)
print(f1_per_class)
array([0.666..., 0.5, 0.8])
What these numbers mean
Class	F1‑score	Explanation
0	0.666…	Model is okay at predicting class 0
1	0.5	Model struggles with class 1
2	0.8	Model is best at class 2
---------------------------
average = macro
(0.666... + 0.5 + 0.8) / 3 = 0.655...
Macro treats all classes equally, even if some appear rarely.
--------------------------------
X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)
make_classification is a data generation utility from sklearn.datasets that creates synthetic datasets for classification problems.
It’s useful for:

Testing algorithms
Prototyping models
Learning machine learning concepts without needing real-world data
The dataset will have 2 target classes (binary classification).
The target variable y will contain only 0 or 1.
-------------------------------------------

https://ml-visualized.com/chapter1/linear_regression
--------------------------------

a) x1.ravel() and x2.ravel()
.ravel() flattens a NumPy array into 1D (row-major order).
If x1 is shape (100, 100), x1.ravel() becomes shape (10000,).
b) np.c_[ ... ]
np.c_ concatenates arrays column-wise.
Here, np.c_[x1.ravel(), x2.ravel()] creates a 2D array where:
First column = flattened x1
Second column = flattened x2
Shape becomes (N, 2) — each row is a coordinate (x1_value, x2_value).

x1 = np.array([1, 2, 3])
x2 = np.array([4, 5, 6])
print(np.c_[x1, x2])
# [[1 4]
#  [2 5]
#  [3 6]]
z = regressor.predict(np.c_[x1.ravel(), x2.ravel()]).reshape(x1.shape)
Then:

x1.ravel() → (10000,)
x2.ravel() → (10000,)
np.c_[...] → (10000, 2)
predict(...) → (10000,)
.reshape(x1.shape) → (100, 100)
----------------------------------------------

X, y = load_breast_cancer(return_X_y=True)
return data and target because return_X_y = True
-------------------------------
https://ompramod.medium.com/decision-trees-91530198a5a5
medium
--------------------------------------------
X = np.sort(5 * np.random.rand(100, 1), axis=0)
100 rows and 1 column
3. np.sort(..., axis=0)
np.sort sorts the array along the specified axis.
axis=0 means sort down each column (vertically).
Since we have only 1 column, it sorts all 100 numbers in ascending order from smallest to largest
---------------------------------
X_grid = np.arange(min(X), max(X), 0.01)[:, np.newaxis]
np.arange(start, stop, step)

Creates a 1D NumPy array starting at start (min(X)), ending before stop (max(X)), with increments of 0.01.
arr = np.array([1, 2, 3])       # shape (3,)
arr[:, np.newaxis]              # shape (3, 1)
# [[1],
#  [2],
#  [3]]
--------------------------------------------
import numpy as np

X = np.zeros((5, 3))  # 5 rows, 3 columns
b = np.random.normal(0, 0.1, X.shape[0])
print(b)
[ 0.056 -0.034  0.102  0.005 -0.087]
1. np.random.normal(mean, std_dev, size)
This function generates random numbers from a normal (Gaussian) distribution.

mean → The center of the distribution.
Here: 0 → values will be centered around 0.

std_dev → Standard deviation (spread of the distribution).
Here: 0.1 → most values will be within ±0.1 of the mean.

size → Number of random values to generate.
Here: X.shape[0] → the number of rows in array X.

------------------------------------------
